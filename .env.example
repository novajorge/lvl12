# ===========================================
# Bender - Environment Variables
# ===========================================
# Copy this file to .env and fill in the values:
#   cp .env.example .env

# -------------------------------------------
# Required: Slack
# -------------------------------------------
SLACK_BOT_TOKEN=xoxb-your-bot-token
SLACK_APP_TOKEN=xapp-your-app-token

# -------------------------------------------
# Required: Claude Code authentication
# Provide at least one of the following:
# -------------------------------------------
ANTHROPIC_API_KEY=sk-ant-your-api-key
# CLAUDE_CODE_OAUTH_TOKEN=your-oauth-token

# -------------------------------------------
# Optional: Bender settings
# -------------------------------------------
BENDER_WORKSPACE=/workspace
BENDER_API_PORT=8080
BENDER_API_KEY=your-secret-api-key
LOG_LEVEL=info

# -------------------------------------------
# Optional: API Mode (Claude, Ollama, MiniMax, NVIDIA, etc.)
# -------------------------------------------
# Option 1: Set BENDER_API_MODE explicitly to skip interactive prompt
# BENDER_API_MODE=claude         # "claude", "ollama", "minimax", "nvidia"

# Option 2: Don't set BENDER_API_MODE (interactive prompt at startup)
# Bender will detect available providers and let you choose

# -------------------------------------------
# Ollama Models (Local LLMs)
# -------------------------------------------
# Configure one or multiple Ollama models
# If multiple models are configured, you'll be asked to choose at startup

# Single model:
# OLLAMA_MODEL=qwen2.5-coder:14b

# Multiple models (will prompt to choose):
# OLLAMA_MODEL_7b=qwen2.5-coder:7b
# OLLAMA_MODEL_14b=qwen2.5-coder:14b
# OLLAMA_MODEL_32b=qwen2.5-coder:32b

# -------------------------------------------
# MiniMax Models
# -------------------------------------------
# Configure one or multiple MiniMax models

# Single model:
# MINIMAX_MODEL=minimax-chat-6b

# Multiple models (will prompt to choose):
# MINIMAX_MODEL_CHAT=minimax-chat-6b
# MINIMAX_MODEL_CODE=minimax-code-6b

# -------------------------------------------
# NVIDIA Build Models
# -------------------------------------------
# Get your API key from: https://build.nvidia.com/settings/api-keys
# Configure one or multiple NVIDIA models

# Single model:
# NVIDIA_MODEL=meta/llama-3.3-70b-instruct-turbo

# Multiple models (will prompt to choose):
# NVIDIA_MODEL_LLAMA_70B=meta/llama-3.3-70b-instruct-turbo
# NVIDIA_MODEL_NEMOTRON_70B=nvidia/llama-3.1-nemotron-70b-instruct
# NVIDIA_MODEL_NEMOTRON_ULTRA=nvidia/llama-3.1-nemotron-ultra-253b-v1
# NVIDIA_MODEL_GEMMA_27B=google/gemma-2-27b-it

# -------------------------------------------
# Custom API Endpoint
# -------------------------------------------
# Required for Ollama and other custom providers
# ANTHROPIC_BASE_URL=http://localhost:11434

# For NVIDIA Build:
# ANTHROPIC_BASE_URL=https://integrate.api.nvidia.com/v1
